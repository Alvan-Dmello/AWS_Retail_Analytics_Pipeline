{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518a032d-377c-4e46-b5e8-d0c88b0665d9",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573a6c05-f755-4ea6-b677-a750801151ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f1b3bc02-247f-428e-8471-e8e689cb0f47;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 597ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f1b3bc02-247f-428e-8471-e8e689cb0f47\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/16ms)\n",
      "24/12/14 21:27:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/14 21:27:54 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Schema of the dataset:\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data Preprocessing Online Store UK\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "s3_path = f\"s3a://project-data-alvan/final_data_v1.csv\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv(s3_path, header=True, inferSchema=True)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    spark.stop()\n",
    "    exit()\n",
    "\n",
    "print(\"Schema of the dataset:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dd95b85-61f6-43a3-b2a5-837685053329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|   543451|    22167| OVAL WALL MIRROR...|       1|  2/8/2011 12:13|    19.96|      NULL|United Kingdom|\n",
      "|   577522|    22944|CHRISTMAS METAL P...|       6|11/20/2011 13:23|     0.39|   15988.0|United Kingdom|\n",
      "|   580367|    22284|HEN HOUSE DECORATION|       1| 12/2/2011 16:39|     3.29|      NULL|United Kingdom|\n",
      "|   576245|    23569|TRADTIONAL ALPHAB...|       4|11/14/2011 13:40|     4.95|   12553.0|        France|\n",
      "|   578293|    22086|PAPER CHAIN KIT 5...|      12|11/23/2011 14:36|     2.95|   15640.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the first 5 rows of the data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cc21b2-df4c-42ef-a67e-8238c12de0c3",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26adc091-c710-4df6-9fd3-601117f4b459",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46c7af2-ad8a-4017-99bc-4ec8906f1288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/09 10:08:01 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 7:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+---------------+-----------------+------------------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|    InvoiceDate|        UnitPrice|        CustomerID|    Country|\n",
      "+-------+------------------+------------------+--------------------+------------------+---------------+-----------------+------------------+-----------+\n",
      "|  count|              4064|              4064|                4056|              4064|           4064|             4064|              3075|       4064|\n",
      "|   mean| 560124.2974191932|27645.306286942505|                NULL|  9.00492125984252|           NULL|4.067093996063008|15360.709268292683|       NULL|\n",
      "| stddev|13290.537207394751| 16692.57366634167|                NULL|22.930074346124094|           NULL|38.23537435510261|1705.3954473312326|       NULL|\n",
      "|    min|            536367|             10002| OVAL WALL MIRROR...|              -390|1/10/2011 10:32|              0.0|           12349.0|  Australia|\n",
      "|    max|           C580971|              POST|ZINC WILLIE WINKI...|               480|  9/9/2011 9:52|          2382.92|           18283.0|Unspecified|\n",
      "+-------+------------------+------------------+--------------------+------------------+---------------+-----------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644b02d0-219d-4124-b7be-775f77027633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|          8|       0|          0|        0|       989|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Count null values in each column\n",
    "null_counts = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d68bef0-c004-4a5a-9266-0ae9a9978a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null values in any column\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "367a5fd5-d358-4a9b-91d4-6f67ff1cbc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|     InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "|   577522|    22944|CHRISTMAS METAL P...|       6|11/20/2011 13:23|     0.39|   15988.0|United Kingdom|\n",
      "|   576245|    23569|TRADTIONAL ALPHAB...|       4|11/14/2011 13:40|     4.95|   12553.0|        France|\n",
      "|   578293|    22086|PAPER CHAIN KIT 5...|      12|11/23/2011 14:36|     2.95|   15640.0|United Kingdom|\n",
      "|   573248|    23247|BISCUIT TIN 50'S ...|       2|10/28/2011 12:09|     2.89|   14498.0|United Kingdom|\n",
      "|  C569985|    22617|BAKING SET SPACEB...|      -3| 10/6/2011 19:51|     4.95|   15365.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+----------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adc9314-6962-45d6-80aa-e66311040d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+------------------+---------------+-----------------+------------------+-----------+\n",
      "|summary|         InvoiceNo|         StockCode|         Description|          Quantity|    InvoiceDate|        UnitPrice|        CustomerID|    Country|\n",
      "+-------+------------------+------------------+--------------------+------------------+---------------+-----------------+------------------+-----------+\n",
      "|  count|              3075|              3075|                3075|              3075|           3075|             3075|              3075|       3075|\n",
      "|   mean| 560810.9327787021| 27448.25352112676|                NULL|11.038373983739838|           NULL|3.790188617886225|15360.709268292683|       NULL|\n",
      "| stddev|12995.291739278331|16305.236310429387|                NULL|24.238442717258327|           NULL|43.21421695917299|1705.3954473312326|       NULL|\n",
      "|    min|            536367|             10120| OVAL WALL MIRROR...|               -36|1/10/2011 10:32|              0.0|           12349.0|  Australia|\n",
      "|    max|           C580971|              POST|ZINC WILLIE WINKI...|               480|  9/9/2011 9:52|          2382.92|           18283.0|Unspecified|\n",
      "+-------+------------------+------------------+--------------------+------------------+---------------+-----------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d165d94-ad4a-43cd-8391-9e5fcfd4e884",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f57869-3628-4d71-9333-667a8b45f2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, month, year, col, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f9233c-eee8-430e-8454-c1c226ae49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'InvoiceDate' from string to timestamp using the specified format\n",
    "df = df.withColumn('InvoiceDate', to_timestamp(col('InvoiceDate'), 'MM/dd/yyyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1d365c-0c99-4b04-9dad-b1ce84c61481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Month and Year\n",
    "df = df.withColumn('Month', month(col('InvoiceDate')))\n",
    "df = df.withColumn('Year', year(col('InvoiceDate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cb9fb2-c8e7-48d1-a343-0807d1786a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+----+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|Month|Year|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+----+\n",
      "|   577522|    22944|CHRISTMAS METAL P...|       6|2011-11-20 13:23:00|     0.39|   15988.0|United Kingdom|   11|2011|\n",
      "|   576245|    23569|TRADTIONAL ALPHAB...|       4|2011-11-14 13:40:00|     4.95|   12553.0|        France|   11|2011|\n",
      "|   578293|    22086|PAPER CHAIN KIT 5...|      12|2011-11-23 14:36:00|     2.95|   15640.0|United Kingdom|   11|2011|\n",
      "|   573248|    23247|BISCUIT TIN 50'S ...|       2|2011-10-28 12:09:00|     2.89|   14498.0|United Kingdom|   10|2011|\n",
      "|  C569985|    22617|BAKING SET SPACEB...|      -3|2011-10-06 19:51:00|     4.95|   15365.0|United Kingdom|   10|2011|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Show the updated DataFrame\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52e7c0e-388d-43aa-9fce-a80f170c261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+----+---------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|Month|Year|MonthName|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+----+---------+\n",
      "|   577522|    22944|CHRISTMAS METAL P...|       6|2011-11-20 13:23:00|     0.39|   15988.0|United Kingdom|   11|2011| November|\n",
      "|   576245|    23569|TRADTIONAL ALPHAB...|       4|2011-11-14 13:40:00|     4.95|   12553.0|        France|   11|2011| November|\n",
      "|   578293|    22086|PAPER CHAIN KIT 5...|      12|2011-11-23 14:36:00|     2.95|   15640.0|United Kingdom|   11|2011| November|\n",
      "|   573248|    23247|BISCUIT TIN 50'S ...|       2|2011-10-28 12:09:00|     2.89|   14498.0|United Kingdom|   10|2011|  October|\n",
      "|  C569985|    22617|BAKING SET SPACEB...|      -3|2011-10-06 19:51:00|     4.95|   15365.0|United Kingdom|   10|2011|  October|\n",
      "|  C557971|    37449|CERAMIC CAKE STAN...|      -1|2011-06-24 10:15:00|     9.95|   18118.0|United Kingdom|    6|2011|     June|\n",
      "|   566301|    21165|BEWARE OF THE CAT...|       1|2011-09-11 16:06:00|     1.69|   16474.0|United Kingdom|    9|2011|September|\n",
      "|   573277|    23419|HOME SWEET HOME B...|       1|2011-10-28 13:18:00|     2.08|   14606.0|United Kingdom|   10|2011|  October|\n",
      "|   551414|   85099B|JUMBO BAG RED RET...|      10|2011-04-28 13:35:00|     2.08|   15622.0|United Kingdom|    4|2011|    April|\n",
      "|   549130|    20676|  RED RETROSPOT BOWL|       6|2011-04-06 15:02:00|     1.25|   14701.0|United Kingdom|    4|2011|    April|\n",
      "|   572909|    23360|SET 8 CANDLES VIN...|       4|2011-10-26 15:48:00|     1.95|   15821.0|United Kingdom|   10|2011|  October|\n",
      "|   546762|    22431|WATERING CAN BLUE...|       1|2011-03-16 14:12:00|     1.95|   17961.0|United Kingdom|    3|2011|    March|\n",
      "|   571667|    22197|      POPCORN HOLDER|       4|2011-10-18 13:04:00|     0.85|   14554.0|United Kingdom|   10|2011|  October|\n",
      "|   557153|    23241|TREASURE TIN GYMK...|       2|2011-06-17 11:07:00|     2.08|   13735.0|United Kingdom|    6|2011|     June|\n",
      "|   553228|    21231|SWEETHEART CERAMI...|      12|2011-05-16 10:48:00|     1.25|   16496.0|United Kingdom|    5|2011|      May|\n",
      "|   549744|    21669|BLUE STRIPE CERAM...|      36|2011-04-12 10:30:00|     1.25|   15240.0|United Kingdom|    4|2011|    April|\n",
      "|   563712|    23256|CHILDRENS CUTLERY...|       4|2011-08-18 15:44:00|     4.15|   12680.0|        France|    8|2011|   August|\n",
      "|   580879|   84519B|CARROT CHARLIE+LO...|       1|2011-12-06 12:18:00|     1.25|   17346.0|United Kingdom|   12|2011| December|\n",
      "|   542812|    21156|RETROSPOT CHILDRE...|      16|2011-02-01 11:21:00|     1.95|   14850.0|United Kingdom|    2|2011| February|\n",
      "|   581221|    23444|   Next Day Carriage|       1|2011-12-08 09:40:00|     15.0|   17856.0|United Kingdom|   12|2011| December|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convert numeric month to alphabetic month name\n",
    "month_dict = {\n",
    "    1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n",
    "    7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'\n",
    "}\n",
    "\n",
    "df = df.withColumn(\"MonthName\", F.expr(\"CASE WHEN month(InvoiceDate) = 1 THEN 'January' \" + \n",
    "                                                    \"WHEN month(InvoiceDate) = 2 THEN 'February' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 3 THEN 'March' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 4 THEN 'April' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 5 THEN 'May' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 6 THEN 'June' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 7 THEN 'July' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 8 THEN 'August' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 9 THEN 'September' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 10 THEN 'October' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 11 THEN 'November' \" +\n",
    "                                                    \"WHEN month(InvoiceDate) = 12 THEN 'December' END\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "609540ee-53e7-4cb7-a6aa-80072191ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45992d62-857b-4467-8f48-d58eeae06471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"MonthName\", \"Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38f7c881-acc6-45d3-80ed-e90948850674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating column revenue\n",
    "df = df.withColumn(\"Revenue\", round(col(\"Quantity\") * col(\"UnitPrice\"),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04f4f4c6-c1ae-41b4-ad3a-803a5b2e9dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----+--------+-------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|Year|   Month|Revenue|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----+--------+-------+\n",
      "|   577522|    22944|CHRISTMAS METAL P...|       6|2011-11-20 13:23:00|     0.39|   15988.0|United Kingdom|2011|November|   2.34|\n",
      "|   576245|    23569|TRADTIONAL ALPHAB...|       4|2011-11-14 13:40:00|     4.95|   12553.0|        France|2011|November|   19.8|\n",
      "|   578293|    22086|PAPER CHAIN KIT 5...|      12|2011-11-23 14:36:00|     2.95|   15640.0|United Kingdom|2011|November|   35.4|\n",
      "|   573248|    23247|BISCUIT TIN 50'S ...|       2|2011-10-28 12:09:00|     2.89|   14498.0|United Kingdom|2011| October|   5.78|\n",
      "|  C569985|    22617|BAKING SET SPACEB...|      -3|2011-10-06 19:51:00|     4.95|   15365.0|United Kingdom|2011| October| -14.85|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+----+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f2be9c-b60f-439d-8e14-7d263a868513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Storing preprocessed to local\n",
    "df.write.csv(\"processed_data\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64485a-acae-470c-92de-cbda7ba50cbe",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e4eee-f131-4725-9017-f77e4965fffd",
   "metadata": {},
   "source": [
    "### 1. Total Revenue by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac752e1-5a0a-4b89-8b93-2db4429d6834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|        Country|      TotalRevenue|\n",
      "+---------------+------------------+\n",
      "| United Kingdom| 51720.96000000006|\n",
      "|           EIRE|2108.0899999999997|\n",
      "|        Germany|1674.2599999999998|\n",
      "|         France|1625.3500000000006|\n",
      "|    Netherlands|1615.0799999999997|\n",
      "|      Australia|           1277.53|\n",
      "|    Switzerland|360.97999999999996|\n",
      "|          Spain|286.65999999999997|\n",
      "|Channel Islands|            205.75|\n",
      "|        Belgium|            173.62|\n",
      "|          Italy|138.29000000000002|\n",
      "|         Norway|            136.89|\n",
      "|       Portugal|            129.38|\n",
      "|        Denmark|             123.0|\n",
      "|         Sweden|             92.59|\n",
      "|        Finland|             83.58|\n",
      "|         Cyprus|              53.6|\n",
      "|         Poland|              51.4|\n",
      "|          Japan|48.650000000000006|\n",
      "| Czech Republic|              45.9|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Calculate total revenue by Country\n",
    "total_revenue_by_region = df.groupBy(\"Country\") \\\n",
    "    .agg(sum(\"Revenue\").alias(\"TotalRevenue\")) \\\n",
    "    .orderBy(col(\"TotalRevenue\").desc())\n",
    "\n",
    "total_revenue_by_region.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaf9320-d63a-489f-ad73-2f8657baf798",
   "metadata": {},
   "source": [
    "### 2. Revenue Growth Over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0271b62d-1358-4cbd-a8c8-ff19b87ffad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|     TotalRevenue|\n",
      "+----+-----------------+\n",
      "|2010|3955.940000000002|\n",
      "|2011|55840.30000000006|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revenue_growth = df.groupBy(\"Year\") \\\n",
    "    .agg(sum(\"Revenue\").alias(\"TotalRevenue\")) \\\n",
    "    .orderBy(\"Year\")\n",
    "\n",
    "revenue_growth.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37903100-7e72-452c-96e3-6cd448ec9a87",
   "metadata": {},
   "source": [
    "### 3. Average Revenue by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adcc5ccc-0d64-4b17-b0e7-a396657f121a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|        Country| AvgMonthlyRevenue|\n",
      "+---------------+------------------+\n",
      "| United Kingdom|3978.5353846153853|\n",
      "|    Netherlands|230.72571428571428|\n",
      "|           EIRE| 162.1607692307692|\n",
      "|      Australia|159.69125000000003|\n",
      "|        Germany|128.78923076923078|\n",
      "|         France|125.02692307692307|\n",
      "|    Switzerland|51.568571428571424|\n",
      "| Czech Republic|              45.9|\n",
      "|Channel Islands|             41.15|\n",
      "|        Denmark|              41.0|\n",
      "|      Lithuania|              35.4|\n",
      "|            USA|              32.4|\n",
      "|       Portugal|            32.345|\n",
      "|        Belgium|28.936666666666667|\n",
      "|          Spain|28.666000000000004|\n",
      "|          Italy|27.658000000000005|\n",
      "|         Norway|27.377999999999997|\n",
      "|         Canada|             26.52|\n",
      "|         Poland|              25.7|\n",
      "|         Sweden|           23.1475|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Calculate average monthly revenue by region\n",
    "avg_monthly_revenue_by_region = df.groupBy(\"Country\", \"Year\", \"Month\") \\\n",
    "    .agg(sum(\"Revenue\").alias(\"MonthlyRevenue\")) \\\n",
    "    .groupBy(\"Country\") \\\n",
    "    .agg(avg(\"MonthlyRevenue\").alias(\"AvgMonthlyRevenue\")) \\\n",
    "    .orderBy(col(\"AvgMonthlyRevenue\").desc())\n",
    "\n",
    "avg_monthly_revenue_by_region.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4b59c-0537-4f50-962e-7a7a615bba26",
   "metadata": {},
   "source": [
    "### 4. Average transaction value per customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b11cab-d641-4666-b811-8ee3e8e599b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|CustomerID|AvgTransactionValue|\n",
      "+----------+-------------------+\n",
      "|   16041.0|              675.0|\n",
      "|   14145.0|              594.0|\n",
      "|   18102.0|             562.66|\n",
      "|   12931.0|              507.5|\n",
      "|   17389.0|             414.48|\n",
      "|   15769.0|              358.0|\n",
      "|   14031.0|             343.76|\n",
      "|   13629.0|              330.0|\n",
      "|   12939.0|             325.44|\n",
      "|   17396.0|              246.1|\n",
      "|   14154.0|              195.0|\n",
      "|   16684.0| 186.42399999999998|\n",
      "|   18092.0|              180.0|\n",
      "|   18064.0|              179.0|\n",
      "|   14607.0|              179.0|\n",
      "|   13093.0|              175.2|\n",
      "|   13798.0|              172.5|\n",
      "|   16553.0|              169.5|\n",
      "|   16029.0|              163.2|\n",
      "|   14680.0|            154.375|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "avg_transaction_value = df.groupBy(\"CustomerID\") \\\n",
    "    .agg((sum(\"Revenue\") / count(\"InvoiceNo\")).alias(\"AvgTransactionValue\")) \\\n",
    "    .orderBy(col(\"AvgTransactionValue\").desc())\n",
    "\n",
    "avg_transaction_value.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa0da6-dafa-4356-a643-9621ff564c6d",
   "metadata": {},
   "source": [
    "### 5. Total Quantity Sold per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f416502-7429-4820-8ce8-e90760f91bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|         Description|TotalQuantitySold|\n",
      "+--------------------+-----------------+\n",
      "|SET OF 60 I LOVE ...|              537|\n",
      "|JUMBO BAG RED RET...|              448|\n",
      "|LUNCH BAG ALPHABE...|              392|\n",
      "|WOODLAND CHARLOTT...|              310|\n",
      "|DOORMAT KEEP CALM...|              306|\n",
      "|DOORMAT UNION JAC...|              304|\n",
      "|MEDIUM CERAMIC TO...|              300|\n",
      "|PACK OF 72 RETROS...|              277|\n",
      "|PLASTERS IN TIN S...|              263|\n",
      "|JAZZ HEARTS ADDRE...|              248|\n",
      "+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total quantity sold per product\n",
    "top_products = df.groupBy(\"Description\") \\\n",
    "    .agg(sum(\"Quantity\").alias(\"TotalQuantitySold\")) \\\n",
    "    .orderBy(col(\"TotalQuantitySold\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "top_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6972421-8590-4b68-aad7-18f828fb7546",
   "metadata": {},
   "source": [
    "## Store Data Back to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328dc2d-35a4-4e6e-8416-b7de89fb74e6",
   "metadata": {},
   "source": [
    "### Storing preprocessed data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91ba1549-3c6e-4dd7-b676-54477f6b85c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 21:36:21 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/12/14 21:36:22 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.csv(\"s3a://project-data-alvan/processed_data\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a7985-d366-455e-b545-37409f01de4d",
   "metadata": {},
   "source": [
    "### Storing aggregated data in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3e0f113-55d3-42f5-aa40-539dfb1fb2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 21:30:50 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/12/14 21:30:51 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "top_products.write.csv(\"s3a://project-data-alvan/aggregated_data/top_products\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d1f520-735b-49d5-b02c-0bbc2745ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 21:31:30 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/12/14 21:31:30 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "avg_transaction_value.write.csv(\"s3a://project-data-alvan/aggregated_data/avg_transaction_value\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b079c621-5d2f-4fc6-9826-597304206e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 21:32:05 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/12/14 21:32:05 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "avg_monthly_revenue_by_region.write.csv(\"s3a://project-data-alvan/aggregated_data/avg_monthly_revenue_by_region\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b756fb0b-7f2f-4fe2-81b0-e045092c6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 21:32:54 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/12/14 21:33:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "revenue_growth.write.csv(\"s3a://project-data-alvan/aggregated_data/revenue_growth\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3cf8809-c85a-4389-b262-2d08fd940fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 21:33:42 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "24/12/14 21:33:55 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "total_revenue_by_region.write.csv(\"s3a://project-data-alvan/aggregated_data/total_revenue_by_region\",header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
